{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zskfYJa8o3qe"
      },
      "source": [
        "5 model comparisons\n",
        "\n",
        "1.   CLIP on original ID images\n",
        "2.   CLIP on  pretrained generated images\n",
        "3.   CLIP on finetuned generated images\n",
        "4.   SD on fintuned embeding\n",
        "5.   SD on pretrained embeding\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wVQiRlPG2XI"
      },
      "source": [
        "#Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9ShzQLGV-CT4",
        "outputId": "2a20957b-0615-43a2-b354-d88c6b3c69f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'personalized-event-classification'...\n",
            "remote: Enumerating objects: 670, done.\u001b[K\n",
            "remote: Counting objects: 100% (302/302), done.\u001b[K\n",
            "remote: Compressing objects: 100% (186/186), done.\u001b[K\n",
            "remote: Total 670 (delta 168), reused 169 (delta 104), pack-reused 368 (from 1)\u001b[K\n",
            "Receiving objects: 100% (670/670), 254.22 KiB | 5.19 MiB/s, done.\n",
            "Resolving deltas: 100% (394/394), done.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.3\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-xnngu496\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-xnngu496\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.19.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=fb5d84252560bdedbeb74d0b12aa7ec908b9f1a8fe86ddbadffa2ce846c1e436\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ym2shw81/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.2.3\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/Reouth/personalized-event-classification.git\n",
        "%pip install -qq git+https://github.com/huggingface/diffusers.git\n",
        "%pip install -q accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IfhQtN8GxYR"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQL5lw8hTsfX",
        "outputId": "399416fb-4ba6-44a2-f212-9093ac8b5a37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9deba63bff154702a8d7e2d0a72c4e99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import configparser\n",
        "from torch import autocast\n",
        "import pandas as pd\n",
        "\n",
        "os.chdir('/content/personalized-event-classification')\n",
        "\n",
        "import SD_model\n",
        "import data_upload\n",
        "import clip_model\n",
        "import helper_functions\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uO1ZfssUZTk"
      },
      "source": [
        "#login access (drive and huggginface)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plukdy70Ur3-"
      },
      "source": [
        "enter Huggingface login token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "baea407a63dc478f859580279d14fcc7"
          ]
        },
        "id": "nDFcYyNXUfD6",
        "outputId": "62da1b68-499f-4259-ce66-2befaf1ba56e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "baea407a63dc478f859580279d14fcc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4), read the license and tick the checkbox if you agree. You have to be a registered user in ğŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "from huggingface_hub import notebook_login\n",
        "!git config --global credential.helper store\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "css5f55wNIjD",
        "outputId": "67fc60f6-3739-4238-edbe-8f0ca32a2360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title connect to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmaKgkX0rV87"
      },
      "source": [
        "#Image classes upload\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXAMEpX2sQV3"
      },
      "source": [
        "\n",
        "the images are loaded from a directory in drive and saved as new name \"{class name (folder)}_{number of frame}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   image_list = [(new_name,PIL image)...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8UStDu64sTNs",
        "outputId": "d9995775-8125-4c99-cc68-f901dcf88298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File /content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/ava is not an image or cannot be opened. Error: [Errno 21] Is a directory: '/content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/ava'\n",
            "File /content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/caleb is not an image or cannot be opened. Error: [Errno 21] Is a directory: '/content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/caleb'\n",
            "File /content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/kyoto is not an image or cannot be opened. Error: [Errno 21] Is a directory: '/content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/kyoto'\n",
            "File /content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/nathan is not an image or cannot be opened. Error: [Errno 21] Is a directory: '/content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/nathan'\n",
            "Entering directory: /content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/ava\n",
            "20 frames in ava class\n",
            "Entering directory: /content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/caleb\n",
            "20 frames in caleb class\n",
            "Entering directory: /content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/kyoto\n",
            "20 frames in kyoto class\n",
            "Entering directory: /content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter/nathan\n",
            "20 frames in nathan class\n"
          ]
        }
      ],
      "source": [
        "data_path = '/content/drive/MyDrive/thesis_OO_SD/ex_machina/frames_GT/red_filter' #folder to dataset image classes\n",
        "image_list =data_upload.upload_images(data_path,class_batch =20)#385"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cWkuifCCwSq"
      },
      "source": [
        "#SD model load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp_UxTSbDrVR"
      },
      "outputs": [],
      "source": [
        "SD_model_name = 'CompVis/stable-diffusion-v1-4'\n",
        "CLIP_model_name = 'openai/clip-vit-large-patch14'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single image diffusion Identifier"
      ],
      "metadata": {
        "id": "wwBQkbE_woxN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq4cggWJC_to"
      },
      "source": [
        "## load from pretrained Stable diffusion model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KKFCrUhWC6TA"
      },
      "outputs": [],
      "source": [
        "# pretrained_models = SD_model.SD_pretrained_load(SD_model_name,CLIP_model_name,device)\n",
        "# pipeline = SD_model.StableDiffusionPipeline(*pretrained_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uzm4bxBFA9S"
      },
      "outputs": [],
      "source": [
        "#@title create text prompt\n",
        "# text = \"a photo of a cat\" #@param {type:\"string\"}\n",
        "# text_embedding = pipeline.text_to_embedding(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8Rjue_R6GDs"
      },
      "outputs": [],
      "source": [
        "#@title ##load pretrained from Imagic trained model\n",
        "# imagic_pretrained_path =  '/content/drive/MyDrive/thesis_OO_SD/Fabelmans_movie/Imagic_embeddings/embeds_with_pipe/a_photo_of_a_person_in_a_movie_scene'\n",
        "# loaded = []\n",
        "# imagic_parameters = data_upload.upload_imagic_params(imagic_pretrained_path,CLIP_model_name,device,loaded)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts7ygUFTmcbA"
      },
      "outputs": [],
      "source": [
        "imagic_pretrained_path =  '/content/drive/MyDrive/thesis_OO_SD/Fabelmans_movie/Imagic_embeddings/embeds_with_pipe/a_photo_of_a_person_in_a_movie_scene'\n",
        "#@title SD classifer\n",
        "alpha = .77 #@param {type:\"number\"}\n",
        "guidance_scale = 8 #@param {type:\"number\"}\n",
        "num_inference_steps = 50 #@param {type:\"number\"}\n",
        "resolution = 512 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "seed = 9 #@param {type:\"number\"}\n",
        "csv_folder = '/content/drive/MyDrive/thesis_OO_SD/Fabelmans_movie/SD_CSV_results'\n",
        "# seed = random.randint(0,200)\n",
        "# for image_name,image,_ in image_list:\n",
        "SD_classified = SD_model.conditioned_classifier(imagic_pretrained_path,CLIP_model_name,device,image_list[0][1],\n",
        "                                          SD_model_name,\n",
        "                                          alpha,\n",
        "                                          seed=seed,\n",
        "                                          height=height,\n",
        "                                          width=width,\n",
        "                                          resolution=resolution,\n",
        "                                          num_inference_steps=num_inference_steps,\n",
        "                                          guidance_scale=guidance_scale\n",
        "                                          )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple images diffusion Identifier"
      ],
      "metadata": {
        "id": "SJIoqSN5xS69"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Baa9sIMY_6WN"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title SD multiple images classifer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "imagic_pretrained_path =   '/content/drive/MyDrive/thesis_OO_SD/ex_machina/Imagic_embeddings/'\n",
        "csv_folder = '/content/drive/MyDrive/thesis_OO_SD/ex_machina/csv_results/statistical_results/no_filter'\n",
        "text_prompts = [\"a_red_filtered_photo_of_a_person\",\"a photo of a person\",\"a photo with a red filter on it\", \"a red filtered photo\"]  #[\"a photo of a person\"]\n",
        "\n",
        "input_dirs=[]\n",
        "output_dirs=[]\n",
        "\n",
        "for text in text_prompts:\n",
        "  input_dirs.append(os.path.join(imagic_pretrained_path,text.replace(\" \",\"_\")))\n",
        "  output_dirs.append(os.path.join(csv_folder,text.replace(\" \",\"_\")))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jy6lh7UhAZ4"
      },
      "outputs": [],
      "source": [
        "for input_dir, output_dir in zip(input_dirs,output_dirs):\n",
        "  os.makedirs(output_dir,exist_ok=True)\n",
        "  alpha = 0 #@param {type:\"number\"}\n",
        "  guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "  num_inference_steps = 50 #@param {type:\"number\"}\n",
        "  resolution = 512 #@param {type:\"number\"}\n",
        "  height = 512 #@param {type:\"number\"}\n",
        "  width = 512 #@param {type:\"number\"}\n",
        "  seed = 9 #@param {type:\"number\"}\n",
        "\n",
        "  # seed = random.randint(0,200)\n",
        "\n",
        "  Imagic_pipe= True\n",
        "  category_class = False\n",
        "  SD_classified = SD_model.all_embeds_conditioned_classifier(input_dir,output_dir, SD_model_name,CLIP_model_name,\n",
        "                                                            device,image_list,category_class,Imagic_pipe,alpha,seed=seed,\n",
        "                                                            height=height,width=width, resolution=resolution,\n",
        "                                                            num_inference_steps=num_inference_steps,guidance_scale=guidance_scale\n",
        "                                                            )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pRH4L8aE-RE"
      },
      "source": [
        "# CLIP clasiffier multiple images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8RLo43LrOpk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title multiple images CLIP Identifier\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "imagic_pretrained_path =   '/content/drive/MyDrive/thesis_OO_SD/ex_machina/generated_images/clip_input/red_filter/imagic'\n",
        "csv_folders = '/content/drive/MyDrive/thesis_OO_SD/ex_machina/csv_results/red_filter/CLIP_imagic_embeds'\n",
        "text_prompts = [\"a_red_filtered_photo_of_a_person\",\"a photo of a person\",\"a photo with a red filter on it\", \"a red filtered photo\"]  #[\"a photo of a person\"]\n",
        "\n",
        "input_dirs=[]\n",
        "output_dirs=[]\n",
        "\n",
        "for text in text_prompts:\n",
        "  input_dirs.append(os.path.join(imagic_pretrained_path,text.replace(\" \",\"_\")))\n",
        "  output_dirs.append(os.path.join(csv_folders,text.replace(\" \",\"_\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5zPFkjPE-RE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = 'ViT-B/32'\n",
        "for image_ID_path, csv_folder in zip(input_dirs,output_dirs):\n",
        "  os.makedirs(csv_folder, exist_ok=True)\n",
        "  clip_pipeline = clip_model.CLIP_pipline(device,model_name)\n",
        "  for image_name,image,_ in image_list:\n",
        "    cls = image_name.rsplit(\"_\",1)[0]\n",
        "    image_flag, df_clip,csv_file_path = helper_functions.csv_checkpoint(csv_folder,cls,image_name)\n",
        "    if image_flag:\n",
        "      continue\n",
        "    else:\n",
        "      clip_embeddings = clip_pipeline.images_to_embeddings(image_ID_path)\n",
        "      clip_classsifed = clip_pipeline.conditioned_classifier(image,clip_embeddings)\n",
        "\n",
        "      df_image = pd.DataFrame(clip_classsifed,columns = ['input_CLIP_embeds','CLIP_loss'])\n",
        "      df_image.insert(0, 'GT Image name', image_name)\n",
        "      df_clip = pd.concat([df_clip, df_image], ignore_index=False)\n",
        "\n",
        "      # Save to CSV\n",
        "      df_clip.to_csv(csv_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoGe_Ey0FpsF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7wVQiRlPG2XI",
        "3IfhQtN8GxYR",
        "_uO1ZfssUZTk",
        "EmaKgkX0rV87",
        "_cWkuifCCwSq",
        "wwBQkbE_woxN",
        "SJIoqSN5xS69",
        "1pRH4L8aE-RE"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}